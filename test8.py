__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')


import os
import streamlit as st
import pandas as pd
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.vectorstores import Chroma
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories.streamlit import StreamlitChatMessageHistory
from langchain_core.documents import Document
from openai import OpenAI



#Chroma tenant ì˜¤ë¥˜ ë°©ì§€ ìœ„í•œ ì½”ë“œ
import chromadb
chromadb.api.client.SharedSystemClient.clear_system_cache()


from langchain_chroma import Chroma

#ì˜¤í”ˆAI API í‚¤ ì„¤ì •
os.environ["OPENAI_API_KEY"] = st.secrets['OPENAI_API_KEY']

# CSV ë¶ˆëŸ¬ì˜¤ê¸° + category ì¶”ê°€ + Documentë¡œ ë³€í™˜
def load_csvs_as_documents(folder_path):
    files = {
        "ìˆ™ë°•": "ëŒ€ì „ê´‘ì—­ì‹œ_ë¬¸í™”ê´€ê´‘(ìˆ™ë°•ì •ë³´)_ì „ì²˜ë¦¬ì™„ë£Œ.csv",
        "ë¡œì»¬ ë§›ì§‘": "ëŒ€ì „ê´‘ì—­ì‹œ_ë¬¸í™”ê´€ê´‘(ëª¨ë²”ìŒì‹ì )_ì „ì²˜ë¦¬ì™„ë£Œ.csv",
        "ì‚¬ì§„ ì°ê¸°": "ëŒ€ì „ê´‘ì—­ì‹œ_ë¬¸í™”ê´€ê´‘(ê´€ê´‘ì§€)_ì „ì²˜ë¦¬ì™„ë£Œ.csv",
        "ì‡¼í•‘": "ëŒ€ì „ê´‘ì—­ì‹œ_ë¬¸í™”ê´€ê´‘(ì‡¼í•‘)_ì „ì²˜ë¦¬ì™„ë£Œ.csv"
    }

    df_list = []
    for category, filename in files.items():
        file_path = os.path.join(folder_path, filename)
        try:
            df = pd.read_csv(file_path, encoding="utf-8")
        except UnicodeDecodeError:
            df = pd.read_csv(file_path, encoding="cp949")
        df["category"] = category
        df_list.append(df)

    merged_df = pd.concat(df_list, ignore_index=True)
    text = merged_df.to_string(index=False)
    documents = [Document(page_content=text, metadata={"source": "í†µí•© CSV"})]

    return documents, merged_df

# ë²¡í„° ì €ì¥ì†Œ ìƒì„±
def create_vector_store(_docs):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    split_docs = text_splitter.split_documents(_docs)
    persist_directory = "./chroma_db"
    vectorstore = Chroma.from_documents(
        split_docs,
        OpenAIEmbeddings(model='text-embedding-3-small'),
        persist_directory=persist_directory
    )
    return vectorstore

# RAG ì²´ì¸ ì´ˆê¸°í™”
def initialize_components(selected_model, docs):
    vectorstore = create_vector_store(docs)
    retriever = vectorstore.as_retriever()

    contextualize_q_prompt = ChatPromptTemplate.from_messages([
        ("system", "Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is."),
        MessagesPlaceholder("history"),
        ("human", "{input}"),
    ])

    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", """You are an assistant for question-answering tasks. \
Use the following pieces of retrieved context to answer the question. \
If you don't know the answer, just say that you don't know. \
Keep the answer perfect. please use imogi with the answer. \
ëŒ€ë‹µì€ í•œêµ­ì–´ë¡œ í•˜ê³ , ì¡´ëŒ“ë§ì„ ì¨ì¤˜.\n\n{context}"""),
        MessagesPlaceholder("history"),
        ("human", "{input}"),
    ])

    llm = ChatOpenAI(model=selected_model)
    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    return rag_chain

# Streamlit UI
st.set_page_config(page_title="ëŒ€ì „ ì—¬í–‰ ì¶”ì²œ ì±—ë´‡")
st.header("ğŸ“ ëŒ€ì „ ë§ì¶¤ ì—¬í–‰ ì½”ìŠ¤ ì¶”ì²œ ì±—ë´‡")

# ëª¨ë¸ ì„ íƒ
option = st.selectbox("Select GPT Model", ("gpt-4o-mini", "gpt-3.5-turbo-0125"))





# ë¡œì»¬ í´ë” ê²½ë¡œ ì„¤ì •
FOLDER_PATH = "data"

if os.path.exists(FOLDER_PATH):
    with st.spinner("ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
        docs, df = load_csvs_as_documents(FOLDER_PATH)
        rag_chain = initialize_components(option, docs)
        chat_history = StreamlitChatMessageHistory(key="chat_messages")

        conversational_rag_chain = RunnableWithMessageHistory(
            rag_chain,
            lambda session_id: chat_history,
            input_messages_key="input",
            history_messages_key="history",
            output_messages_key="answer",
        )

        if "messages" not in st.session_state:
            st.session_state["messages"] = [{"role": "assistant", "content": "ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!"}]

        for msg in chat_history.messages:
            st.chat_message(msg.type).write(msg.content)

        if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
            st.chat_message("human").write(prompt)
            with st.chat_message("ai"):
                with st.spinner("Thinking..."):
                    config = {"configurable": {"session_id": "default"}}
                    response = conversational_rag_chain.invoke(
                        {"input": prompt},
                        config
                    )
                    st.write(response['answer'])
                    with st.expander("ì°¸ê³  ë¬¸ì„œ ë³´ê¸°"):
                        for doc in response['context']:
                            st.markdown(doc.metadata.get('source', 'ì•Œ ìˆ˜ ì—†ìŒ'), help=doc.page_content)
else:
    st.error(f"âŒ í´ë” ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:\n{FOLDER_PATH}")


# 
options = st.multiselect(
    "ì–´ë–¤ ê±¸ ì¤‘ì‹¬ìœ¼ë¡œ ì—¬í–‰í•˜ê³  ì‹¶ë‚˜ìš”?",
    ["ìˆ™ë°•", "ì‚¬ì§„ ì°ê¸°", "ë¡œì»¬ ë§›ì§‘", "ì‡¼í•‘"]
)

# OpenAI í´ë¼ì´ì–¸íŠ¸
client = OpenAI()

if options:
    # CSVì—ì„œ ì„ íƒëœ ì˜µì…˜ í•„í„°ë§
    filtered_context = ""
    for opt in options:
        filtered_df = df[df['category'].str.contains(opt, na=False)]
        for _, row in filtered_df.iterrows():
            filtered_context += f"{row['name']} ({row['address']})\n"

    # GPT-MINIì—ê²Œ ìš”ì²­
    prompt = f"""
    ë„ˆëŠ” ëŒ€ì „ ì—¬í–‰ ì „ë¬¸ê°€ì•¼.
    ì‚¬ìš©ìê°€ {', '.join(options)} í…Œë§ˆë¡œ ì—¬í–‰ì„ í•˜ê³  ì‹¶ëŒ€.
    ì•„ë˜ ëŒ€ì „ ê´€ê´‘ì§€ ë°ì´í„° ì°¸ê³ í•´ì„œ,
    ì¶”ì²œ ì—¬í–‰ ì½”ìŠ¤ì™€ ì¼ì • ë§Œë“¤ì–´ì¤˜.

    ê´€ê´‘ì§€ ëª©ë¡:
    {filtered_context}
    """
	
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "user", "content": prompt}
        ]
    )

    # ê²°ê³¼ ì¶œë ¥
    st.markdown(response.choices[0].message.content)
else:
    st.info("ê´€ì‹¬ ìˆëŠ” í…Œë§ˆë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”!")
