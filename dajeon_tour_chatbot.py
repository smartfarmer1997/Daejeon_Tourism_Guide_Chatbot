import os
import streamlit as st
import pandas as pd
import shutil

from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.vectorstores import Chroma
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories.streamlit import StreamlitChatMessageHistory
from langchain_core.documents import Document
from openai import OpenAI

#Chroma tenant ì˜¤ë¥˜ ë°©ì§€ ìœ„í•œ ì½”ë“œ
import chromadb
chromadb.api.client.SharedSystemClient.clear_system_cache()

__import__('pysqlite3')
import sys
sys.modules['sqlite3'] = sys.modules.pop('pysplite3')

from langchain_chroma import Chroma

from chromadb.config import Settings

client_settings = Settings(chroma_db_impl="duckdb+parquet", persist_directory=persist_directory)

vectorstore = Chroma.from_documents(
    split_docs,
    OpenAIEmbeddings(model="text-embedding-3-small"),
    persist_directory=persist_directory,
    client_settings=client_settings
)

# ì˜¤í”ˆAI API í‚¤ ì„¤ì •
os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]

st.set_page_config(page_title="ëŒ€ì „ ì—¬í–‰ ì¶”ì²œ ì±—ë´‡")
st.header("ğŸ“ ëŒ€ì „ ë§ì¶¤ ì—¬í–‰ ì½”ìŠ¤ ì¶”ì²œ ì±—ë´‡")

# ğŸ“„ ì‚¬ì´ë“œë°”ì—ì„œ PDF ì—…ë¡œë“œ ë°›ê¸°
with st.sidebar:
    st.markdown("ğŸ“„ PDF íŒŒì¼ ì—…ë¡œë“œ")
    uploaded_pdf = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["pdf"])

# ğŸ“„ PDF ë¡œë”© ë° ë²¡í„° ë””ë ‰í† ë¦¬ ì§€ì •
if uploaded_pdf:
    temp_path = os.path.join("./data", uploaded_pdf.name)
    with open(temp_path, "wb") as f:
        f.write(uploaded_pdf.getvalue())
    pdf_loader = PyPDFLoader(temp_path)
    pdf_docs = pdf_loader.load()
    file_key = uploaded_pdf.name.replace(".pdf", "").replace(" ", "_")
    persist_directory = f"./chroma_db_{file_key}"
else:
    default_pdf_path = os.path.join("./data", "ê¿€ì¼ë„ì‹œëŒ€ì „ê°€ì´ë“œë¶_ì›¹ë°°í¬ìš©.pdf")
    pdf_loader = PyPDFLoader(default_pdf_path)
    pdf_docs = pdf_loader.load()
    persist_directory = "./chroma_db_default"

# ğŸ“Š CSV ë¡œë“œ
def load_csvs_as_documents(folder_path):
    files = {
        "ìˆ™ë°•": "ëŒ€ì „ê´‘ì—­ì‹œ_ë¬¸í™”ê´€ê´‘(ìˆ™ë°•ì •ë³´)_ì „ì²˜ë¦¬ì™„ë£Œ.csv",
        "ë¡œì»¬ ë§›ì§‘": "ëŒ€ì „ê´‘ì—­ì‹œ_ë¬¸í™”ê´€ê´‘(ëª¨ë²”ìŒì‹ì )_ì „ì²˜ë¦¬ì™„ë£Œ.csv",
        "ì‚¬ì§„ ì°ê¸°": "ëŒ€ì „ê´‘ì—­ì‹œ_ë¬¸í™”ê´€ê´‘(ê´€ê´‘ì§€)_ì „ì²˜ë¦¬ì™„ë£Œ.csv",
        "ì‡¼í•‘": "ëŒ€ì „ê´‘ì—­ì‹œ_ë¬¸í™”ê´€ê´‘(ì‡¼í•‘)_ì „ì²˜ë¦¬ì™„ë£Œ.csv"
    }
    df_list = []
    for category, filename in files.items():
        file_path = os.path.join(folder_path, filename)
        try:
            df = pd.read_csv(file_path, encoding="utf-8")
        except UnicodeDecodeError:
            df = pd.read_csv(file_path, encoding="cp949")
        df["category"] = category
        df_list.append(df)
    merged_df = pd.concat(df_list, ignore_index=True)
    text = merged_df.to_string(index=False)
    documents = [Document(page_content=text, metadata={"source": "í†µí•© CSV"})]
    return documents, merged_df

FOLDER_PATH = "./data"
docs, df = load_csvs_as_documents(FOLDER_PATH)
all_docs = pdf_docs + docs

# ğŸ” ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
def create_vector_store(_docs, persist_directory):
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    split_docs = splitter.split_documents(_docs)
    vectorstore = Chroma.from_documents(
        split_docs,
        OpenAIEmbeddings(model="text-embedding-3-small"),
        persist_directory=persist_directory
    )
    return vectorstore

def get_vector_store(_docs, persist_directory):
    if os.path.exists(persist_directory):
        return Chroma(
            persist_directory=persist_directory,
            embedding_function=OpenAIEmbeddings(model="text-embedding-3-small")
        )
    else:
        return create_vector_store(_docs, persist_directory)

# ğŸ”— RAG ì²´ì¸ ì´ˆê¸°í™”
def initialize_components(selected_model, docs, persist_directory):
    vectorstore = get_vector_store(docs, persist_directory)
    retriever = vectorstore.as_retriever()

    contextualize_q_prompt = ChatPromptTemplate.from_messages([
        ("system", "Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is."),
        MessagesPlaceholder("history"),
        ("human", "{input}"),
    ])

    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", """You are an assistant for question-answering tasks. \
       Use the following pieces of retrieved context to answer the question. \
       If you don't know the answer, just say that you don't know. \
       Keep the answer perfect. please use imogi with the answer. \
       ëŒ€ë‹µì€ í•œêµ­ì–´ë¡œ í•˜ê³ , ì¡´ëŒ“ë§ì„ ì¨ì¤˜. \
       ë§Œì•½ ì‚¬ìš©ìê°€ ë‚ ì”¨ë‚˜ í™œë™ ì¶”ì²œ, ì‹¤ë‚´/ì‹¤ì™¸ ì„ íƒ ë“±ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì„ í•œë‹¤ë©´, ì—…ë¡œë“œëœ PDF ë‚´ìš©(ì˜ˆ: ê¸°ìƒì •ë³´ ë¬¸ì„œ, ê°€ì´ë“œë¶ ë“±)ì„ ì ê·¹ì ìœ¼ë¡œ ì°¸ê³ í•´ì„œ ë‹µë³€í•´ì¤˜.\n\n{context}"""),
        MessagesPlaceholder("history"),
        ("human", "{input}"),
    ])

    llm = ChatOpenAI(model=selected_model)
    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    return rag_chain

# ğŸ§  ëª¨ë¸ ì„ íƒ ë° ì²´ì¸ ì‹¤í–‰
option = st.selectbox("Select GPT Model", ("gpt-4o-mini", "gpt-3.5-turbo-0125"))

if os.path.exists(FOLDER_PATH):
    with st.spinner("ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
        rag_chain = initialize_components(option, all_docs, persist_directory)
        chat_history = StreamlitChatMessageHistory(key="chat_messages")

        conversational_rag_chain = RunnableWithMessageHistory(
            rag_chain,
            lambda session_id: chat_history,
            input_messages_key="input",
            history_messages_key="history",
            output_messages_key="answer",
        )

        if "messages" not in st.session_state:
            st.session_state["messages"] = [{"role": "assistant", "content": "ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!"}]

        for msg in chat_history.messages:
            st.chat_message(msg.type).write(msg.content)

        if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”"):
            st.chat_message("human").write(prompt)
            with st.chat_message("ai"):
                with st.spinner("Thinking..."):
                    config = {"configurable": {"session_id": "default"}}
                    response = conversational_rag_chain.invoke({"input": prompt}, config)
                    st.write(response["answer"])
                    with st.expander("ì°¸ê³  ë¬¸ì„œ ë³´ê¸°"):
                        for doc in response["context"]:
                            st.markdown(doc.metadata.get("source", "ì•Œ ìˆ˜ ì—†ìŒ"), help=doc.page_content)
else:
    st.error(f"âŒ í´ë” ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {FOLDER_PATH}")

# ğŸ¯ ì„ íƒ ê¸°ë°˜ ì½”ìŠ¤ ì¶”ì²œ
options = st.multiselect("ì–´ë–¤ ê±¸ ì¤‘ì‹¬ìœ¼ë¡œ ì—¬í–‰í•˜ê³  ì‹¶ë‚˜ìš”?", ["ìˆ™ë°•", "ì‚¬ì§„ ì°ê¸°", "ë¡œì»¬ ë§›ì§‘", "ì‡¼í•‘"])
client = OpenAI()

if options:
    filtered_context = ""
    for opt in options:
        filtered_df = df[df["category"].str.contains(opt, na=False)]
        for _, row in filtered_df.iterrows():
            filtered_context += f"{row['name']} ({row['address']})\n"

    prompt = f"""
    ë„ˆëŠ” ëŒ€ì „ ì—¬í–‰ ì „ë¬¸ê°€ì•¼.
    ì‚¬ìš©ìê°€ {', '.join(options)} í…Œë§ˆë¡œ ì—¬í–‰ì„ í•˜ê³  ì‹¶ëŒ€.
    ì•„ë˜ ëŒ€ì „ ê´€ê´‘ì§€ ë°ì´í„° ì°¸ê³ í•´ì„œ,
    ì¶”ì²œ ì—¬í–‰ ì½”ìŠ¤ì™€ ì¼ì • ë§Œë“¤ì–´ì¤˜.

    ê´€ê´‘ì§€ ëª©ë¡:
    {filtered_context}
    """

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}]
    )

    st.markdown(response.choices[0].message.content)
else:
    st.info("ê´€ì‹¬ ìˆëŠ” í…Œë§ˆë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”!")
